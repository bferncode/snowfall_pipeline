{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fb15cd8-8f6d-4e8e-b24c-68024489506a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Enables autoreload; learn more at https://docs.databricks.com/en/files/workspace-modules.html#autoreload-for-python-modules\n",
    "# To disable autoreload; run %autoreload 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68acf050-bb90-44b7-b609-5e243de5c76c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9619f74-53d7-41c6-8f80-f67777ec9ff7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from modules.clients import SnotelClient, NWSClient\n",
    "from modules.database import upsert_to_delta, get_table_watermark\n",
    "from modules.transformer import generate_combined_forecast\n",
    "from config.settings import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed58958e-3100-4ca2-b337-2b968a6a074a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Setup Parameters\n",
    "run_mode = \"full\"  # options: [\"incremental\", \"full\"]\n",
    "\n",
    "snotel = SnotelClient()\n",
    "nws = NWSClient()\n",
    "\n",
    "# 2. Determine Dates\n",
    "if run_mode == \"incremental\":\n",
    "    last_date = get_table_watermark(TBL_SNOW_OBS, \"date\")\n",
    "    start_date = (last_date).strftime('%Y-%m-%d') if last_date else \"2025-12-01\"\n",
    "    end_date = (pd.Timestamp.now() + pd.Timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "else:\n",
    "    start_date = \"2025-12-01\"\n",
    "    end_date = pd.Timestamp.now().strftime('%Y-%m-%d')\n",
    "\n",
    "print(f\"Running in {run_mode} mode with dates: {start_date} to {end_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49ad0012-e6ea-4375-8076-c4e37bc24ea8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3. Process Observations\n",
    "stations = spark.table(TBL_WEATHER_STATIONS).toPandas()\n",
    "# stations = stations[:5]\n",
    "all_obs = []\n",
    "for _, row in stations.iterrows():\n",
    "    data = snotel.fetch_historical_data(row['site_id'], row['ntwk'], start_date, end_date)\n",
    "    if data is not None: \n",
    "        all_obs.append(data)\n",
    "\n",
    "if all_obs:\n",
    "    from modules.transformer import transform_historical_data\n",
    "    \n",
    "    # Combine raw dataframes\n",
    "    obs_df = pd.concat(all_obs)\n",
    "    \n",
    "    # Transform raw columns (Station Id -> site_id, etc.) to match Delta schema\n",
    "    # This addresses the [DELTA_MERGE_UNRESOLVED_EXPRESSION] error\n",
    "    clean_obs_df = transform_historical_data(obs_df)\n",
    "    \n",
    "    # Use 'date_hr' and 'site_id' as join keys for a more granular unique constraint\n",
    "    upsert_to_delta(clean_obs_df, TBL_SNOW_OBS, [\"date_hr\", \"site_id\"], mode=run_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d2d1db6-440f-4fe4-a0ad-a957e06814b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4. Process Forecasts\n",
    "all_hourly = []\n",
    "all_snow_grid = []\n",
    "\n",
    "# Determine write mode for forecasts\n",
    "# If run_mode is \"incremental\", we use \"upsert\" to update existing windows\n",
    "forecast_write_mode = \"upsert\" if run_mode == \"incremental\" else \"overwrite\"\n",
    "\n",
    "for _, row in stations.iterrows():\n",
    "    # Fetch data\n",
    "    hourly_data = nws.get_hourly_forecast(row['lat'], row['lon']).assign(site_id=row['site_id'])\n",
    "    snow_data = nws.get_snow_grid_data(row['lat'], row['lon']).assign(site_id=row['site_id'])\n",
    "    \n",
    "    all_hourly.append(hourly_data)\n",
    "    all_snow_grid.append(snow_data)\n",
    "\n",
    "# Apply the dynamic mode to satisfy incremental update requirements\n",
    "if all_hourly:\n",
    "    upsert_to_delta(pd.concat(all_hourly), TBL_WEATHER_FCST, [\"startTime\", \"site_id\"], mode=forecast_write_mode)\n",
    "\n",
    "if all_snow_grid:\n",
    "    upsert_to_delta(pd.concat(all_snow_grid), TBL_SNOW_FCST, [\"snow_start\", \"site_id\"], mode=forecast_write_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d34c80a-d2bb-4137-a302-87f82cd4caeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 5. Aggregate (Efficient Spark Join)\n",
    "generate_combined_forecast()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e111c932-5773-4dfd-8730-209148ca9c99",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1767910769990}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from gold.weather.combined_forecast where site_id = '505';\n",
    "-- select * from bronze.raw.snowfall_observations where site_id = '505';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2db8b68-46b9-4197-ac57-de425176ccb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "with depth as (\n",
    "select\n",
    "  site_id,\n",
    "  snow_depth_in\n",
    "from (\n",
    "  select\n",
    "    site_id,\n",
    "    snow_depth_in,\n",
    "    row_number() over (partition by site_id order by date_hr desc) as rn\n",
    "  from bronze.raw.snowfall_observations\n",
    ")\n",
    "where rn = 1\n",
    ")\n",
    "select * from depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71d95468-6059-4f35-ab43-2156feb34a95",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"snow_depth_in\":152},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1767911764426}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "with depth as (\n",
    "select\n",
    "  site_id,\n",
    "  snow_depth_in\n",
    "from (\n",
    "  select\n",
    "    site_id,\n",
    "    snow_depth_in,\n",
    "    row_number() over (partition by site_id order by date_hr desc) as rn\n",
    "  from bronze.raw.snowfall_observations\n",
    ")\n",
    "where rn = 1\n",
    ")\n",
    "select\n",
    "  f.site_name,\n",
    "  s.lat,\n",
    "  s.lon,\n",
    "  depth.snow_depth_in,\n",
    "  sum(f.total_snow_in) as snow_acc\n",
    "from\n",
    "  gold.weather.combined_forecast as f\n",
    "    join bronze.raw.weather_stations as s\n",
    "      on s.site_id = f.site_id\n",
    "    left join depth\n",
    "      on cast(depth.site_id as string) = f.site_id\n",
    "where\n",
    "  snow_end < '2026-01-11'\n",
    "group by\n",
    "  all\n",
    "order by\n",
    "  snow_acc desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "652d777c-14c1-4e9a-b38d-497ceac03bf5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_df = _sqldf.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80ec6a6b-d77b-4a7f-951b-7eb7bfae7631",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "defc36c9-cfcf-4933-b206-f632b84c0c9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Create the scatter mapbox visual\n",
    "fig = px.scatter_mapbox(\n",
    "    plot_df, \n",
    "    lat=\"lat\", \n",
    "    lon=\"lon\", \n",
    "    color=\"snow_acc\",              # Markers colored by snow accumulation\n",
    "    size=\"snow_acc\",               # Marker size relative to snow amount (optional)\n",
    "    # color_continuous_scale=px.colors.sequential.Ice, # Color scale representing snow\n",
    "    hover_name=\"site_name\",        # Header for the tooltip\n",
    "    hover_data={                   # Customize tooltip content\n",
    "        \"lat\": False,              # Hide lat/lon in tooltip if redundant\n",
    "        \"lon\": False, \n",
    "        \"snow_acc\": \":.2f\"         # Show snow_acc with 2 decimal places\n",
    "    },\n",
    "    zoom=6,                        # Starting zoom level for Colorado/mountain regions\n",
    "    center={\"lat\": 39.0, \"lon\": -106.0}, # Centered near CO mountains\n",
    "    height=600,\n",
    "    title=\"Forecasted Snow Accumulation by Station\"\n",
    ")\n",
    "\n",
    "# Update layout to use an open-source map style\n",
    "fig.update_layout(\n",
    "    mapbox_style=\"carto-positron\", \n",
    "    margin={\"r\":0,\"t\":40,\"l\":0,\"b\":0}\n",
    ")\n",
    "\n",
    "# Render in Databricks\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6894664365297195,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "etl_notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
